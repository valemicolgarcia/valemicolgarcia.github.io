---
layout: default
title: Stress Detection from Text
---

<section class="project-detail-section">
  <div class="project-header">
    <a href="/roadmap.html" class="back-link"><i class="fas fa-arrow-left"></i> Back to Roadmap</a>
    <h1 class="project-detail-title">Stress Detection from Text</h1>
    <p class="project-detail-subtitle">Natural Language Processing (NLP)</p>
    <div class="project-tags">
      <span class="tag">NLP</span>
      <span class="tag">Logistic Regression</span>
      <span class="tag">WordCloud</span>
      <span class="tag">Text Classification</span>
      <span class="tag">NLTK</span>
    </div>
    <div class="github-link">
      <a href="https://github.com/valemicolgarcia/valemicolgarcia.github.io/blob/main/projects/Estres/nlp-regresionLogistica.ipynb" target="_blank" class="github-btn">
        <i class="fab fa-github"></i> View code on GitHub
      </a>
      <a href="/projects/Estres/test-stress.html" class="test-btn">
        <i class="fas fa-flask"></i> TRY IT NOW!
      </a>
    </div>
  </div>

  <div class="project-content">
    <div class="project-overview">
      <h2>Problem to Solve</h2>
      <p>This project uses natural language processing (NLP) and machine learning techniques to detect stress levels in written texts. The system takes unstructured text as input, preprocesses it, and applies classification models to determine the probability that the text reflects stress. Interactive visualizations help explore data patterns and model results.</p>
    </div>

    <div class="dataset-section">
      <h2>1. Dataset Information</h2>
      <p class="section-description">The dataset contains texts extracted from different Reddit subreddits, with information about the stress level detected in each text.</p>
      
      <div class="variables-info">
        <div class="variables-category">
          <h3>Dataset Columns</h3>
          <ul>
            <li><strong>subreddit:</strong> Specific Reddit community or forum</li>
            <li><strong>post_id:</strong> Unique post identifier</li>
            <li><strong>sentence_range:</strong> Sentence index within the post</li>
            <li><strong>text:</strong> Text used to detect stress</li>
            <li><strong>label:</strong> 0 means "no stress", 1 means "stress"</li>
            <li><strong>confidence:</strong> Person's confidence level in the text</li>
            <li><strong>social_timestamp:</strong> Timestamp recording when the post was published</li>
          </ul>
        </div>
      </div>

      <div class="dataset-example">
        <h3>Dataset Example</h3>
        <div class="table-container">
          <table class="data-table">
            <thead>
              <tr>
                <th>subreddit</th>
                <th>post_id</th>
                <th>sentence_range</th>
                <th>text</th>
                <th>label</th>
                <th>confidence</th>
                <th>social_timestamp</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>ptsd</td>
                <td>8601tu</td>
                <td>(15, 20)</td>
                <td>He said he had not felt that way before, sugge...</td>
                <td>1</td>
                <td>0.8</td>
                <td>1521614353</td>
              </tr>
              <tr>
                <td>assistance</td>
                <td>8lbrx9</td>
                <td>(0, 5)</td>
                <td>Hey there r/assistance, Not sure if this is th...</td>
                <td>0</td>
                <td>1.0</td>
                <td>1527009817</td>
              </tr>
              <tr>
                <td>ptsd</td>
                <td>9ch1zh</td>
                <td>(15, 20)</td>
                <td>My mom then hit me with the newspaper and it s...</td>
                <td>1</td>
                <td>0.8</td>
                <td>1535935605</td>
              </tr>
              <tr>
                <td>relationships</td>
                <td>7rorpp</td>
                <td>[5, 10]</td>
                <td>until i met my new boyfriend, he is amazing, h...</td>
                <td>1</td>
                <td>0.6</td>
                <td>1516429555</td>
              </tr>
              <tr>
                <td>survivorsofabuse</td>
                <td>9p2gbc</td>
                <td>[0, 5]</td>
                <td>October is Domestic Violence Awareness Month a...</td>
                <td>1</td>
                <td>0.8</td>
                <td>1539809005</td>
              </tr>
            </tbody>
          </table>
        </div>
      </div>
    </div>

    <div class="text-analysis-section">
      <h2>2. Text Analysis</h2>
      <p class="section-description">An exploratory analysis of the texts was performed to understand their characteristics and distributions. The average words per text is <strong>85 words</strong>.</p>
      
      <div class="analysis-images-grid">
        <div class="analysis-image-item">
          <h3>Words per Review</h3>
          <img src="{{ site.baseurl }}/projects/Estres/palabrasporrevision.png" alt="Word distribution per text" />
        </div>
        <div class="analysis-image-item">
          <h3>String Distribution</h3>
          <img src="{{ site.baseurl }}/projects/Estres/distribucionStrings.png" alt="String distribution" />
        </div>
        <div class="analysis-image-item">
          <h3>General WordCloud</h3>
          <img src="{{ site.baseurl }}/projects/Estres/wordcloudGeneral.png" alt="General dataset WordCloud" />
        </div>
      </div>
    </div>

    <div class="nltk-section">
      <h2>3. Processing with NLTK</h2>
      <p class="section-description">NLTK (Natural Language Toolkit) is a Python library that provides tools for working with natural language data. It was used to preprocess and normalize texts before model training.</p>
      
      <div class="preprocessing-step">
        <h3>3.1 Normalization and Tokenization</h3>
        <p>All words were converted to lowercase and texts were tokenized (divided into individual words). By removing duplicates, the number of unique tokens was reduced by <strong>10%</strong> with normalization.</p>
        
        <div class="code-example">
          <pre><code>from nltk import word_tokenize

token_lists = [word_tokenize(each) for each in df['text']]
tokens = [item for sublist in token_lists for item in sublist]

print("Tokens √∫nicos antes: ", len(set(tokens)))

token_lists_lower = [word_tokenize(each) for each in df['text_new']]
tokens_lower = [item for sublist in token_lists_lower for item in sublist]
print("Tokens √∫nicos nuevos: ", len(set(tokens_lower)))</code></pre>
        </div>
      </div>

      <div class="preprocessing-step">
        <h3>3.2 Special Character Removal</h3>
        <p>Special characters that do not provide information for classification were removed, such as emojis, symbols, and excessive punctuation marks.</p>
        <div class="special-chars-list">
          <p><strong>Removed characters:</strong> {'"', 'üê∞', ''', 'üíï', '>', '\u200d', '+', '_', '\\', '‚û°', '\t', '\u200e', 'üôÇ', ''', '¬∑', '‚Ä¶', '#', '‚óè', 'üéì', '‚Ç¨', '(', "'", '<', '"', '^', '¬¥', 'ü•ï', 'üòî', 'üò¶', ':', '"', '/', '?', '‚ù§', '‚Äì', '%', 'üë©', '@', 'Ô∏è', 'üòá', '[', '‚Äî', '-', '!', 'üí∏', '$', '¬Ø', '.', ')', '&', ',', '¬£', '=', '‚Ä¢', ';', '~', ']', '*'}</p>
        </div>
      </div>

      <div class="preprocessing-step">
        <h3>3.3 Lemmatization</h3>
        <p>Lemmatization is the process of reducing a word to its base form or lemma. For example, "running", "runs", and "ran" become "run". This helps normalize variations of the same word.</p>
        
        <div class="lemmatization-example">
          <div class="example-before">
            <h4>Before:</h4>
            <p class="example-text">"I am feeling anxious and worried. My heart is racing and I cannot stop thinking about problems."</p>
          </div>
          <div class="example-after">
            <h4>After:</h4>
            <p class="example-text">"I be feel anxious and worry. My heart be race and I cannot stop think about problem."</p>
          </div>
        </div>

        <div class="code-example">
          <pre><code>from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

lemmatizer = WordNetLemmatizer()

def lemmatize_text(text):
    words = word_tokenize(text)
    lemmatized_words = [lemmatizer.lemmatize(word, wordnet.VERB) for word in words]
    return ' '.join(lemmatized_words)

X_lema = X.apply(lemmatize_text)</code></pre>
        </div>
      </div>

      <div class="preprocessing-step">
        <h3>3.4 Removal of Stopwords and High/Low Frequency Words</h3>
        <p>Words that do not provide information for classification were removed:</p>
        
        <div class="frequency-analysis">
          <div class="frequency-card">
            <h4>High Frequency Words (removed)</h4>
            <p>These words appear very frequently but do not provide information:</p>
            <ul class="frequency-list">
              <li><strong>('i', 13907)</strong> - appears 13,907 times</li>
              <li><strong>('to', 8315)</strong> - appears 8,315 times</li>
              <li><strong>('and', 7954)</strong> - appears 7,954 times</li>
              <li><strong>('the', 6236)</strong> - appears 6,236 times</li>
              <li><strong>('a', 5339)</strong> - appears 5,339 times</li>
              <li><strong>('my', 4471)</strong> - appears 4,471 times</li>
              <li><strong>('of', 3634)</strong> - appears 3,634 times</li>
              <li><strong>('it', 3521)</strong> - appears 3,521 times</li>
              <li><strong>('that', 3038)</strong> - appears 3,038 times</li>
              <li><strong>('me', 3036)</strong> - appears 3,036 times</li>
            </ul>
          </div>
          
          <div class="frequency-card">
            <h4>Low Frequency Words (removed)</h4>
            <p>These words appear only once and also do not provide information:</p>
            <ul class="frequency-list">
              <li>('labyrinth', 1)</li>
              <li>('bureaucracy', 1)</li>
              <li>('squeeze', 1)</li>
              <li>('wayne', 1)</li>
              <li>('guzzler', 1)</li>
              <li>('lightheadedness', 1)</li>
              <li>('extremities', 1)</li>
              <li>('radiates', 1)</li>
              <li>('disassociation', 1)</li>
              <li>('usd', 1)</li>
            </ul>
          </div>
        </div>

        <div class="code-example">
          <pre><code>import nltk
nltk.download('stopwords')

eng_stop_words = nltk.corpus.stopwords.words('english')
noise_words = list(eng_stop_words)

processed_stopwords = [word.lower() for stopword in noise_words 
                       for word in word_tokenize(stopword)]</code></pre>
        </div>
      </div>

      <div class="preprocessing-step">
        <h3>3.5 Train-Test Split</h3>
        <p>The dataset was split before vectorizing to more easily compare original texts with preprocessed ones. This allows maintaining a clear reference of the original data.</p>
      </div>

      <div class="preprocessing-step">
        <h3>3.6 Vectorization (Bag of Words)</h3>
        <p>Vectorization is the process of converting text into numbers that the machine learning model can process. <strong>Bag of Words (BOW)</strong> represents each text as a vector where each position corresponds to a word in the vocabulary, and the value indicates how many times that word appears in the text.</p>
        
        <div class="code-example">
          <pre><code>bow_counts = CountVectorizer(
    tokenizer=word_tokenize,
    stop_words=processed_stopwords,
    ngram_range=(1, 2)  # bigramas
)

X_train_bow = bow_counts.fit_transform(X_train)
X_test_bow = bow_counts.transform(X_test)</code></pre>
        </div>
        
        <div class="vectorization-explanation">
          <p><strong>Why bigrams?</strong> Bigrams capture pairs of consecutive words (like "I am", "am stressed"), which helps capture context and relationships between words that are important for detecting stress.</p>
        </div>
      </div>
    </div>

    <div class="wordcloud-section">
      <h2>4. Wordclouds Before Training</h2>
      <p class="section-description">Wordclouds show the most frequent words in each category, revealing distinctive linguistic patterns between stressed and non-stressed texts.</p>
      
      <div class="wordcloud-container">
        <div class="wordcloud-item wordcloud-full">
          <h3>Comparison: Stressed vs Non-Stressed People</h3>
          <div class="wordcloud-image">
            <img src="{{ site.baseurl }}/projects/Estres/stressvsNotstressed.png" alt="WordCloud stressed vs non-stressed people" />
            <p class="wordcloud-note">The visual comparison shows differences in word patterns. In texts from stressed people, words like <strong>"anxiety"</strong>, <strong>"friend"</strong>, <strong>"work"</strong>, <strong>"need"</strong>, <strong>"back"</strong>, <strong>"time"</strong>, and <strong>"know"</strong> appear more frequently and in different contexts than in texts from non-stressed people.</p>
          </div>
        </div>
      </div>
    </div>

    <div class="model-section">
      <h2>5. Model Training</h2>
      <p class="section-description">A Logistic Regression model was trained using stratified cross-validation (StratifiedKFold) to robustly evaluate performance.</p>
      
      <div class="model-results-section">
        <h3>Model Results</h3>
        <div class="metrics-display-new">
          <div class="metric-card-new">
            <div class="metric-header">
              <div class="metric-icon">Acc</div>
              <div class="metric-title">Accuracy</div>
            </div>
            <div class="metric-value-new">0.71</div>
            <div class="metric-desc-new">Average model accuracy</div>
          </div>
          
          <div class="metric-card-new">
            <div class="metric-header">
              <div class="metric-icon">Pre</div>
              <div class="metric-title">Precision</div>
            </div>
            <div class="metric-value-new">0.71</div>
            <div class="metric-desc-new">Average precision (StratifiedKFold)</div>
          </div>
          
          <div class="metric-card-new">
            <div class="metric-header">
              <div class="metric-icon">Rec</div>
              <div class="metric-title">Recall</div>
            </div>
            <div class="metric-value-new">0.71</div>
            <div class="metric-desc-new">Average recall (StratifiedKFold)</div>
          </div>
          
          <div class="metric-card-new">
            <div class="metric-header">
              <div class="metric-icon">F1</div>
              <div class="metric-title">F1 Score</div>
            </div>
            <div class="metric-value-new">0.71</div>
            <div class="metric-desc-new">Average F1 Score (StratifiedKFold)</div>
          </div>
        </div>
      </div>
    </div>

    <div class="test-section">
      <h2>6. Model Testing</h2>
      <p class="section-description">The model was tested with real examples to verify its functionality:</p>
      
      <div class="test-examples">
        <div class="test-item test-stressed">
          <div class="test-icon">
            <i class="fas fa-exclamation-triangle"></i>
          </div>
          <h3>"I am really stressed and anxious"</h3>
          <div class="test-result">
            <span class="prediction-label">Prediction:</span>
            <span class="prediction-stress">1 (Stressed)</span>
          </div>
          <p class="test-explanation">The model correctly identifies that this phrase indicates the presence of stress.</p>
        </div>
        
        <div class="test-item test-relaxed">
          <div class="test-icon">
            <i class="fas fa-smile"></i>
          </div>
          <h3>"I am relaxed"</h3>
          <div class="test-result">
            <span class="prediction-label">Prediction:</span>
            <span class="prediction-no-stress">0 (Not stressed)</span>
          </div>
          <p class="test-explanation">The model correctly identifies that this phrase indicates the absence of stress.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
